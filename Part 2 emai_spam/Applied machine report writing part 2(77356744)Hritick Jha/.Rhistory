####set k=5 ####
k <- 5
set.seed(123)
kmeans_result <- kmeans(email_spam_test.s, centers = k, nstart = 25)
email_spam_test$cluster <- as.factor(kmeans_result$cluster)
table(email_spam_test$cluster)
if(ncol(email_spam_test.s) <= 3) {
library(scatterplot3d)
scatterplot3d(email_spam_test.s[,1:3], color = kmeans_result$cluster, pch = 19)
} else {
print("Too many dimensions to plot.")
}
print(kmeans_result$centers)
print(table(email_spam_test$cluster))
par(mar = c(2, 2, 1, 1))
options(repr.plot.width = 2, repr.plot.height = 1)
plot(email_spam_test, col = setk$cluster)
par(mar = c(5, 4, 4, 2) + 0.1)
plot(email_spam_test, col = setk$cluster)
####set k=5 ####
k <- 5
set.seed(123)
kmeans_result <- kmeans(email_spam_test.s, centers = k, nstart = 25)
email_spam_test$cluster <- as.factor(kmeans_result$cluster)
table(email_spam_test$cluster)
f(ncol(email_spam_test.s) <= 3) {
if(ncol(email_spam_test.s) <= 3) {
library(scatterplot3d)
scatterplot3d(email_spam_test.s[,1:3], color = kmeans_result$cluster, pch = 19)
} else {
print("Too many dimensions to plot.")
}
print(kmeans_result$centers)
print(table(email_spam_test$cluster))
par(mar = c(5, 4, 4, 2) + 0.1)
plot(email_spam_test, col = email_spam_test$cluster)
####set k=5 ####
k <- 5
set.seed(123)
kmeans_result <- kmeans(email_spam_test.s, centers = k, nstart = 25)
email_spam_test$cluster <- as.factor(kmeans_result$cluster)
table(email_spam_test$cluster)
if(ncol(email_spam_test.s) <= 3) {
library(scatterplot3d)
scatterplot3d(email_spam_test.s[,1:3], color = kmeans_result$cluster, pch = 19)
} else {
print("Too many dimensions to plot.")
}
print(kmeans_result$centers)
print(table(email_spam_test$cluster))
par(mar = c(5, 4, 4, 2) + 0.1)
plot(email_spam_test, col = email_spam_test$cluster)
####set k=5 ####
k <- 5
set.seed(123)
kmeans_result <- kmeans(email_spam_test.s, centers = k, nstart = 25)
email_spam_test$cluster <- as.factor(kmeans_result$cluster)
table(email_spam_test$cluster)
if(ncol(email_spam_test.s) <= 3) {
library(scatterplot3d)
scatterplot3d(email_spam_test.s[,1:3], color = kmeans_result$cluster, pch = 19)
} else {
print("Too many dimensions to plot.")
}
print(kmeans_result$centers)
print(table(email_spam_test$cluster))
par(mar = c(5, 4, 4, 2) + 0.1)
plot(email_spam_test, col = email_spam_test$cluster, main = "Clustering Results with k=5")
####set k=5 ####
k <- 5
set.seed(123)
kmeans_result <- kmeans(email_spam_test.s, centers = k, nstart = 25)
email_spam_test$cluster <- as.factor(kmeans_result$cluster)
table(email_spam_test$cluster)
if(ncol(email_spam_test.s) <= 3) {
library(scatterplot3d)
scatterplot3d(email_spam_test.s[,1:3], color = kmeans_result$cluster, pch = 19)
} else {
print("Too many dimensions to plot.")
}
print(kmeans_result$centers)
print(table(email_spam_test$cluster))
par(mar = c(2, 2, 1, 1))
options(repr.plot.width = 2, repr.plot.height = 1)
par(mar = c(5, 4, 4, 2) + 0.1)
plot(email_spam_test, col = setk$cluster)
#### Hirearchical clustering #####
hc <- hclust(dist(email_spam_test.s))
plot(hc, cex = 0.6, main = "Dendrogram of Hierarchical Clustering")
#### Hirearchical clustering #####
hc <- hclust(dist(email_spam_test.s))
plot(hc, cex = 0.6, main = "Dendrogram of Hierarchical Clustering email_spam_test")
#### Cluster distance ####
cluster_dist <- as.dendrogram(hc)
plot(cluster_dist, main = "Cluster Distance emai_spam_test", cex = 0.6)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(mlbench)
data("PimaIndiansDiabetes2")
diabetes<-PimaIndiansDiabetes2
logit <- glm(diabetes~glucose, family = binomial,data = diabetes)
summary(logit)
library(tidyverse)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(glmnet)
email_spam_test <- read.csv("email_spam_test.csv")
email_spam_test <- email_spam_test[, !(names(email_spam_test) %in% c("Email 24", "Email 18", "Email 15"))]
names(email_spam_test)[names(email_spam_test) == "the"] <- "target"
names(email_spam_test)[names(email_spam_test) == "to"] <- "text"
email_spam_test$target <- as.factor(email_spam_test$target)
numeric_cols <- sapply(email_spam_test, is.numeric)
glm_model <- glm(target ~ ., data = email_spam_test, family = binomial)
names(email_spam_test)[names(email_spam_test) == "text"] <- "email_text"
email_spam_test$target <- as.factor(email_spam_test$target)
numeric_cols <- sapply(email_spam_test, is.numeric)
glm_model <- glm(target ~ ., data = email_spam_test, family = binomial)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(tidyverse)
glm_model <- glm(target ~ glucose, family = binomial, data = email_spam_test)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(tidyverse)
logit <- glm(target ~ num_words, family = binomial, data = email_spam_test)
logit <- glm(target ~ theme(the), family = binomial, data = email_spam_test)
logit <- glm(target ~ the, family = binomial, data = email_spam_test)
logit <- glm(target ~ correct_variable_name, family = binomial, data = email_spam_test)
logit <- glm(target ~ the, family = binomial, data = email_spam_test)
logit <- glm(target ~ theme(), family = binomial, data = email_spam_test)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(mlbench)
data("email_spam_test")
df <- read.csv("email_spam_test.csv")
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(mlbench)
df <- read.csv("email_spam_test.csv")
diabetes<-email_spam_test
logit <- glm(diabetes~target,family = binomial, data = diabetes)
logit <- glm(target ~ ., family = binomial, data = diabetes)
logit <- glm(target ~ ., family = binomial, data = diabetes)
diabetes <- diabetes[, !duplicated(names(diabetes))]
logit <- glm(target ~ ., family = binomial, data = diabetes)
logit <- glm(target ~ num_words, family = binomial, data = email_spam_test)
logit <- glm(target ~ Theoph, family = binomial, data = email_spam_test)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(mlbench)
df <- read.csv("email_spam_test.csv")
diabetes<-email_spam_test2
diabetes<-email_spam_test
logit <- glm(diabetes~glucose, family = binomial,data = diabetes)
logit <- glm(target ~ num_words, family = binomial, data = df)
names(df)
logit <- glm(target ~ num_characters, family = binomial, data = df)
logit <- glm(target ~ num_words, family = binomial, data = df)
logit <- glm(target ~ Theoph(wt), family = binomial, data = df)
df$num_characters <- nchar(df$to)
str(df)
names(df)
logit <- glm(target ~ num_characters, family = binomial, data = df)
unique(df$target)
df$num_characters <- nchar(df$to)
str(df)
names(df)
logit <- glm(target ~ num_characters, family = binomial, data = df)
unique(df$target)
df$target <- factor(df$target)
unique(df$target)
logit <- glm(target ~ num_characters, family = binomial, data = df)
summary(logit)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(mlbench)
df <- read.csv("email_spam_test.csv")
diabetes<-email_spam_test
unique(df$target)
df$target <- factor(df$target)
unique(df$target)
logit <- glm(target ~ num_characters, family = binomial, data = df)
df$num_characters <- nchar(df$text)
logit <- glm(target ~ num_characters, family = binomial, data = df)
summary(logit)
library(tidyverse)
test<-mutate(diabetes, prob = ifelse(diabetes == "pos", 1, 0))
train<-mutate(diabetes, prob = ifelse(diabetes == "pos", 1, 0))
duplicate_names <- names(df)[duplicated(names(df))]
print(duplicate_names)
new_column_names <- make.unique(names(df))
names(df) <- new_column_names
train <- mutate(diabetes, prob = ifelse(diabetes == "pos", 1, 0))
duplicate_names <- names(diabetes)[duplicated(names(diabetes))]
print(duplicate_names)
new_column_names <- make.unique(names(diabetes))
names(diabetes) <- new_column_names
train <- mutate(diabetes, prob = ifelse(diabetes == "pos", 1, 0))
ggplot(train, aes(num_characters, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Plasma number_character Concentration",
y = "Probability of being diabete-pos"
)
ggplot(train, aes(num_characters, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Plasma number_character Concentration",
y = "Probability of being spam"
)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(mlbench)
df <- read.csv("email_spam_test.csv")
diabetes<-email_spam_test
unique(df$target)
df$target <- factor(df$target)
unique(df$target)
df$num_characters <- nchar(df$text)
logit <- glm(target ~ num_characters, family = binomial, data = df)
summary(logit)
library(tidyverse)
duplicate_names <- names(diabetes)[duplicated(names(diabetes))]
print(duplicate_names)
new_column_names <- make.unique(names(diabetes))
names(diabetes) <- new_column_names
train <- mutate(diabetes, prob = ifelse(diabetes == "pos", 1, 0))
ggplot(train, aes(x = num_characters, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Number of Characters",
y = "Probability of being spam"
)
ggplot(df, aes(x = num_characters, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Number of Characters",
y = "Probability of being spam"
)
ggplot(df, aes(x = num_characters, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Number of Characters",
y = "Probability of being spam"
)
library(mlbench)
data("PimaIndiansDiabetes2")
diabetes<-PimaIndiansDiabetes2
logit <- glm(diabetes~glucose, family = binomial,data = diabetes)
summary(logit)
library(tidyverse)
train2<-mutate(diabetes, prob = ifelse(diabetes == "pos", 1, 0))
ggplot(train2, aes(glucose, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Plasma Glucose Concentration",
y = "Probability of being diabete-pos"
)
ggplot(train2, aes(glucose, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Number of characters",
y = "Probability of being diabete-pos"
)
####supervised machine learning logistic Regression ####
####Logistic Regression In R ####
library(mlbench)
df <- read.csv("email_spam_test.csv")
diabetes<-email_spam_test
unique(df$target)
df$target <- factor(df$target)
unique(df$target)
df$num_characters <- nchar(df$text)
logit <- glm(target ~ num_characters, family = binomial, data = df)
summary(logit)
library(tidyverse)
duplicate_names <- names(diabetes)[duplicated(names(diabetes))]
print(duplicate_names)
new_column_names <- make.unique(names(diabetes))
names(diabetes) <- new_column_names
train <- mutate(diabetes, prob = ifelse(diabetes == "pos", 1, 0))
ggplot(df, aes(x = num_characters, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Number of Characters",
y = "Probability of being spam"
)
ggplot(train2, aes(x = glucose, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "Plasma Glucose Concentration",
y = "Probability of being diabetes-positive"
)
ggplot(train, aes(x =num_cols, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model in R",
x = "Num_characters",
y = "Probability of being diabetes-positive"
)
ggplot(train, aes(x =target, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model in R",
x = "Num_characters",
y = "Probability of being diabetes-positive"
)
ggplot(train, aes(x =diabetes, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model in R",
x = "Num_characters",
y = "Probability of being diabetes-positive"
)
ggplot(train, aes(x = num_characters, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model in R",
x = "Number of Characters",
y = "Probability of being diabetes-positive"
)
library(ggplot2)
ggplot(train, aes(x = num_characters, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model in R",
x = "Number of Characters",
y = "Probability of being diabetes-positive"
)
ggplot(train, aes(x = num_characters, y = prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model in R",
x = "Number of Characters",
y = "Probability of being diabetes-positive"
);
ggplot(train2, aes(glucose, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model",
x = "num_characters",
y = "Probability of being diabete-pos"
)
ggplot(train2, aes(glucose, prob)) +
geom_point(alpha = 0.2) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
labs(
title = "Logistic Regression Model in R",
x = "num_characters",
y = "Probability "
)
####Multiple Logistic Regression ####
logit2 <- glm(diabetes~., family = binomial,data = diabetes)
summary(logit2)
#### Logistic regression using Caret package ####
library(caret)
#### Data Split ####
indexes<-sample(1:nrow(diabetes), 4/5*nrow(diabetes))
#### Data Split ####
indexes<-sample(1:nrow(target), 4/5*nrow(num_characters))
####Multiple Logistic Regression ####
logit2 <- glm(diabetes~., family = binomial,data = diabetes)
####Multiple Logistic Regression ####
logit <- glm(target ~ num_characters, family = binomial, data = df)
summary(logit2)
#### Logistic regression using Caret package ####
library(caret)
#### Data Split ####
indexes <- sample(1:nrow(email_spam_test), 4/5 * nrow(email_spam_test))
train <- email_spam_test[indexes, ]
test <- email_spam_test[-indexes, ]
prop.table(table(train$target)) * 100
prop.table(table(test$target)) * 100
#### Training Model ####
train_cleaned <- na.omit(train)
caret_glm_mod <- train(
form = target ~ .,
data = train_cleaned,
trControl = trainControl(method = "cv", number = 5),
method = "glm",
family = "binomial"
)
caret_glm_mod <- train(
form = target ~ num_characters,
data = train_cleaned,
trControl = trainControl(method = "cv", number = 5),
method = "glm",
family = "binomial"
)
#### Training Model ####
train_cleaned <- na.omit(train)
caret_glm_mod <- train(
form = target ~ num_characters,
data = train_cleaned,
trControl = trainControl(method = "cv", number = 5),
method = "glm",
family = "binomial"
)
train_cleaned$num_characters <- nchar(train_cleaned$text)
#### Training Model ####
indexes <- sample(1:nrow(email_spam_test), 4/5 * nrow(email_spam_test))
train <- email_spam_test[indexes, ]
test <- email_spam_test[-indexes, ]
prop.table(table(train$target)) * 100
prop.table(table(test$target)) * 100
train_cleaned <- na.omit(train)
caret_glm_mod <- train(
form = target ~ num_characters,
data = train_cleaned,
trControl = trainControl(method = "cv", number = 5),
method = "glm",
family = "binomial"
)
caret_glm_mod <- train(
form = target ~ num_characters,
data = train_cleaned,
trControl = trainControl(method = "df", number = 5),
method = "glm",
family = "binomial"
)
caret_glm_mod <- train(
form = target ~ numeric_data,
data = train_cleaned,
trControl = trainControl(method = "df", number = 5),
method = "glm",
family = "binomial"
)
caret_glm_mod <- train(
form = target ~ .,  # Use all numeric predictors
data = train_cleaned,
trControl = trainControl(method = "cv", number = 5),
method = "glm",
family = "binomial"
)
install.packages("mice")
install.packages("mice")
library(mice)
imputed_data <- mice(train, method = 'pmm', m = 1, maxit = 5)
train_imputed <- complete(imputed_data, 1)
caret_glm_mod <- train(
form = target ~ .,
data = train_imputed,
trControl = trainControl(method = "cv", number = 5),
method = "glm",
family = "binomial"
)
#### supervised Machine learning K Nearest Neighbour[KNN] ####
#### Transformation - normalizing numeric data #####
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize(c(1, 2, 3, 4, 5))
normalize(c(10, 20, 30, 40, 50))
wbcd_norm <- as.data.frame(lapply(wbcd[2:31], normalize))
summary(wbcd_norm$area_mean)
####supervised Machine learning K Nearest Neighbour Regression####
install.packages("MASS")
install.packages("MASS")
library(MASS)
boston<-Boston
miss<-apply(boston, 2, function(x) sum(is.na(x)))
miss
summary(boston)
boston_norm <- as.data.frame(lapply(boston[1:13], normalize))
boston_norm$Med_price<-boston$medv
library(caret)
set.seed(123)
y <- boston_norm$Med_price
indxTrain <- createDataPartition(y, p = 0.7, list = FALSE)
boston_train <- boston_norm[indxTrain,]
boston_test <- boston_norm[-indxTrain,]
model <- train(
Med_price~., data = boston_train, method = "knn",
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
plot(model)
#### Model Evaluation ####
library(caret)
pred<-predict(Boston.rf, test)
metrics_rmse = RMSE(pred,test$medv)
metrics_r2 = R2(pred, test$medv)
metrics_MEA = MAE(pred, test$medv)
c(metrics_rmse,metrics_r2,metrics_MEA )
df<-data.frame(pred=pred, obs=test$medv)
library(ggplot2)
ggplot(df, aes(x=obs, y=pred))+geom_point()
